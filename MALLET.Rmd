### Set Up Environment

Load up required packages.

```{r}
setwd("~/Downloads/MALLET-Lesson")
rm(list=ls())
library(mallet) # a wrapper around the Java machine learning tool MALLET
library(wordcloud) # to visualize wordclouds
```

## 0. Prepare

First let's read in our data. The corpus comes from ORACC ([The Open Richly Annotated Cuneiform Corpus](http://oracc.museum.upenn.edu)), which hosts the State Archives of Assyria Online. Each entry contains the transcription of a tablet and its bibliographic information.

```{r}
#read in CSV file
documents <- read.csv("SAA.csv", stringsAsFactors = F)
names(documents)
```

## 1. Estimate Mallet Topics

```{r}

# load data into mallet
# the list of stopwords is empty for now, but we'll revisit it later
mallet.instances <- mallet.import(documents$Designation, documents$Text, "Data/stoplist.csv", FALSE, token.regexp="[\\p{L}']+")

# Decide what number of topics to model
n.topics = 10

## Create a topic trainer object.
topic.model <- MalletLDA(n.topics)

## Load our documents
topic.model$loadDocuments(mallet.instances)

## Get the vocabulary, and some statistics about word frequencies.
##  These may be useful in further curating the stopword list.
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)

# examine some of the vocabulary
word.freqs[1:10,]

## Optimize hyperparameters every 10 iterations, 
##  after 200 burn-in iterations.
topic.model$setAlphaOptimization(10, 200)

## Now train a model. Note that hyperparameter optimization is on, by default.
## We can specify the number of iterations to train the model.
## Typically 1000-5000 iterations is appropriate.
topic.model$train(2500)

## Get the probability of topics in documents and the probability of words in topics.
## By default, these functions return raw word counts. Here we want probabilities, 
##  so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)

## What are the top words in topic 7?
## Notice that R indexes from 1, so this will be the topic that mallet called topic 6.
mallet.top.words(topic.model, topic.words[6,])

## Get a vector containing short names for the topics
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")

## Have a look at keywords for each topic
topics.labels
```

## 2. Revising Our Model

For a variety of reasons, we typically remove stopwords ('to', 'of', 'and', etc) from our documents before feeding them into the topic model. These words appear with high frequency in many of our documents and potentially create noise in the patterns we are looking for.

```
# Sort the list of words in our model by the number of documents in which they appear
order.freq <- order(word.freqs$doc.freq, decreasing = TRUE)
ordered.words <- word.freqs[order.freq,]

# Inspect the most frequent words in our corpus
head(ordered.words)

# For more comprehensive treatment, we can export our word list to a CSV.
# After inspecting that by hand, we may wish to add terms to the stopword file.
write.csv(ordered.words, 'Word-Frequencies.csv'
```

In his study of literary theme, Matt Jockers has suggested that it may be useful to include only nouns when topic modeling. Since the corpus includes a POS-tag with each word, it is relatively easy to filter out other parts of speech. (We will also filter our proper nouns, which is a standard practice.)

```
# Create a new list, in which each entry will be a list of the words from each tablet
word.list.list <- list()

for (text in 1:length(documents$Text))
{
  word.list.list[text] <- strsplit(documents$Text[[text]], ' ')
}

head(word.list.list)


# Create a new list, in which each entry will be a single character string containing the nouns from each tablet
word.list.filtered <- list()

for (text in 1:length(word.list.list))
{
    this.word.list <- word.list.list[text][[1]]
    new.list <- list()
    
    for (word in 1:length(this.word.list))
    {
        this.word <- this.word.list[word]
        tag.index <- gregexpr(']', this.word)[[1]][1] + 1
        tag <- substr(this.word, tag.index, nchar(this.word))
        if (tag=='N')
        {
            new.list[length(new.list)+1] <- this.word
        }
	
    }
    new.char <- paste(new.list, collapse = ' ')
    word.list.filtered[length(word.list.filtered)+1] <- new.char
}

head(word.list.filtered)

documents$Text.Nouns <- word.list.filtered
```

Often when we work with very short texts like tablets or poems, it is useful to aggregate these into a few larger texts. However, the basis on which we decide to aggregate can have interpretive consequences. Do we collect all of the tablets by a given author? All of the tablets from a certain region? 

In this case, we will gather texts by their chapter in the SAA. Ostensibly, these have already been curated by theme (e.g. "Letters to the King"). In a sense, we will use our topic model to evaluate each chapter's degree of thematic unity!

## 3. Evaluate Our Model

```
## Show the first few document titles with at least .25 of its content devoted to topic 4
head(documents$title[ doc.topics[4,] > 0.25 ],10)

## Show title of the most representative text for topic 4
documents[which.max(doc.topics[4,]),]$title
